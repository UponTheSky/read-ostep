# Pagingg: Faster Translations (TLBs)
- paging requires a large amount of mapping information
  - paging logically requires an extra memory lookup for each virtual address generated by the program
  
- solution: **TLB**(translation-lookaside buffer)
  - a part of the chip's MMU(memory management unit)
  - it is simply a HW cache of popular virtual-to-physical address translations
  - upon each virtual memory reference, the HW first checks the TLB to see if the desired translation is held there

## 1. TLB Basic Algorithm
- how (roughly) TLB algorithm works?
  - first, we extract VPN from the virtual address, and check if that VPN is already in the TLB
  - if there is, we have a *TLB hit*: extract PFN from the TLB entry and generate PA with the offset from the virtual address
  - otherwise(*TLB miss*), the HW accesses the page table to find the translation and updates the TLB with that translation; then HW retries the instruction

- TLB miss is costly: 
  - memory accesses, relative to most CPU instructions are expensive
  - TLB misses lead to more memory access

## 2. Example: Accessing An Array
- spatial locality(hitting the data in vicinity), temporal locality(hitting the same data in near time)
- the role that page size(= block size in cache) plays

## 3. Who Handles The TLB Miss?
- modern architectures prefer SW managed TLBs
  - on a TLB miss, the HW raises an exception
  - then the privilege level is raised to kernel mode, and we jump to a trap handler
  - then this trap handler deals with TLB misses(looking up the page table, update TLB, etc.)
  - then the trap handler returns to the instruction that raised the exception, not the next instruction
  (compared to syscall exceptions)

- remark: the TLB handler must be preserved properly, to avoid an infinite chain of TLB misses
  - so that the handler must always get a TLB hit

- why SW-management is preferred?
  - *flexible*: any data structure the OS want can be used, without changing HWs
  - *simple*: the most of jobs are handed over to the SW(OS)

## 4. TLB Contents: Whatâ€™s In There?
- TLB cache is **fully-associative**: any given translation can be anywhere in the TLB
  - the HW search the entire TLB in parallel to find the desired translation
  - the VPN and PFN(PPN) are present in each entry(a translation could end up in any of these entries)

- other bits:
  - **valid** bit: if set, the entry has a valid translation within the entry
  - **protection** bit: r/w/x
  - **address-space identifier**, **dirty** bits

## 5. TLB Issue: Context Switches
- TLB addresses are only valid for the currently running process
- the HW and OS must avoid the situation in which the next process accidently access the previous process's TLB address
- solution
  - flushing the entire TLBs: but this is costly since every time a newly resuming process must begin with TLB misses
  - sharing the TLB across context switches
    - HW provides an **address space identifier**(ASID) field in the TLB
    - so by looking at ASID, the process can recognize what entry is valid for itself
    - HW supports a special privileged register to the ASID of the current process

## 6. Issue: Replacement Policy
- cache replacement policy for minimizing cache misses
- **LRU(least-recently-used)**, random

## 7. A Real TLB Entry
- example: MIPS R4000
- read through this part

## HW
- see `code/tlb.c` and `trial.js` for details
- 5: `volatile` declaration
- 6: cannot find a way for macos
- 7: possibly not including the first trial(solely for initializing the array, against the effect of demand paging)